# -*- coding: utf-8 -*-
"""Web_Crawler.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JC0DFVO5t6yxJZb25Zf3dm86VwKofRaG
"""

!pip install requests beautiful-soup4

import requests
from bs4 import BeautifulSoup
import os

class Crawler:
    def __init__(self):
        self.session = requests.Session()

    def get_soup(self, url):
        try:
            response = self.session.get(url, timeout=10)
            if response.status_code == 200:
                html = response.content
                # Save to a file
                filename = url.replace('https://', '').replace('http://', '').replace('/', '_') + '.html'
                if not os.path.exists('downloaded_pages'):
                    os.makedirs('downloaded_pages')
                with open(os.path.join('downloaded_pages', filename), 'wb') as f:
                    f.write(html)
                return BeautifulSoup(html, 'html.parser')
            elif response.status_code == 301:
                print(f'Redirected: {response.url}')
                return None
            elif response.status_code == 403:
                print('Forbidden')
                return None
            elif response.status_code == 404:
                print('Not Found')
                return None
            elif response.status_code == 500:
                print('Internal Server Error')
                return None
            else:
                print(f'Unexpected HTTP response: {response.status_code}')
                return None
        except requests.exceptions.RequestException as e:
            print(f'RequestException: {str(e)}')
            return None

    def crawl(self, url, max_depth=3):
        if max_depth < 0:
            return
        print(f'Depth: {max_depth} - Visiting: {url}')
        soup = self.get_soup(url)
        if soup is None:
            print("Failed to get page")
            return
        for link in soup.find_all('a'):
            href = link.get('href')
            if href and href.startswith('http'):
                self.crawl(href, max_depth-1)

if __name__ == "__main__":
    crawler = Crawler()
    crawler.crawl("https://example.com", 2)

#Crawl the Wikipedia
crawler = Crawler()
crawler.crawl("https://en.wikipedia.org/wiki/Main_Page", 3)

#Crawl Python Wevbsite
crawler = Crawler()
crawler.crawl("https://www.python.org", 1)

#Crawl the New York Times website 
crawler = Crawler()
crawler.crawl("https://www.nytimes.com", 2)

#Crawl the GitHub homepage
crawler = Crawler()
crawler.crawl("https://github.com", 1)